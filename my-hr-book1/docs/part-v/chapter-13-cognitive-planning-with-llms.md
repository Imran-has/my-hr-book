# Chapter 13: Cognitive Planning with LLMs

Traditional robotics task planners are often based on rigid, formal logic systems like PDDL (Planning Domain Definition Language). While powerful, these systems require a manually-crafted, symbolic representation of the world and all possible actions. This is brittle and does not scale well to the complexity and unpredictability of the real world.

Large Language Models (LLMs) offer a revolutionary alternative. By leveraging the vast "common sense" knowledge embedded in their training data, LLMs can act as high-level **cognitive planners**, breaking down complex, abstract goals into a sequence of simpler, executable steps.

### The LLM as a Task Planner

The core idea is to treat the LLM as a reasoning engine. Instead of asking it to write a story, you ask it to create a plan.

Consider the user command: `"clean up the kitchen"`.

A traditional planner would fail unless it had a pre-programmed, step-by-step definition of what "clean up the kitchen" means. An LLM, however, can use its world knowledge to infer a plausible sequence of actions.

**Example Interaction:**

**System Prompt to LLM:**
```
You are a helpful robot assistant. Your task is to break down high-level commands into a sequence of simple actions that you can perform. The available actions are:
- NavigateTo(location)
- DetectObject(object_name)
- Grasp(object_id)
- Place(object_id, location)
- ThrowAway(object_id)

Given the command "clean up the kitchen", provide a plan.
```

**LLM Response (Plan):**
```json
[
  {"action": "NavigateTo", "params": ["kitchen_counter"]},
  {"action": "DetectObject", "params": ["soda_can"]},
  {"action": "Grasp", "params": ["soda_can_123"]},
  {"action": "ThrowAway", "params": ["soda_can_123"]},
  {"action": "DetectObject", "params": ["dirty_plate"]},
  {"action": "Grasp", "params": ["dirty_plate_456"]},
  {"action": "NavigateTo", "params": ["dishwasher"]},
  {"action": "Place", "params": ["dirty_plate_456", "dishwasher"]}
]
```

This approach is powerful because it allows the robot to handle novel commands it has never seen before, as long as the LLM can reason about them using its existing knowledge.

### Grounding the Plan

The plan generated by the LLM is still symbolic. The robot's **execution system** is responsible for **grounding** these symbols in reality.

- When the plan says `NavigateTo(kitchen_counter)`, the execution system must call the actual navigation controller (like Nav2) with the known map coordinates of the kitchen counter.
- When the plan says `DetectObject(soda_can)`, the system must run its perception module to find an object that matches the "soda can" class in its camera feed.

### Challenges in LLM-based Planning

- **Hallucination:** The LLM might generate a plan that involves objects that don't exist or actions the robot cannot perform. The execution system must be able to detect when a step fails and potentially re-prompt the LLM for a new plan ("I could not find a soda can. What should I do instead?").
- **State Tracking:** The LLM itself is stateless. The robotics system needs to maintain a representation of the world's state and feed it back to the LLM when it needs to make a decision.

This chapter will provide a practical guide to implementing an LLM-based planning system, using a prompt engineering strategy to constrain the LLM's output and an execution engine that translates the LLM's plan into ROS 2 actions.